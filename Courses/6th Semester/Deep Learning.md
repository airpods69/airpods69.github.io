#6th-Semester 

- [x] **Deep Networks**:Â 
	- [x] Artificial Neural Networks
	- [x] Choice of Activation Functions
	- [x] Gradient-based Learning
	- [x] Hidden Units
	- [x] Architecture Design
	- [x] Backward Propagation and Differentiation
	- [x] Forward/Backward Propagation
	- [x] Computational Graphs
	- [x] Chain Rule in Back-propagation
	- [x] Stochastic Gradient Descent Algorithms
	- [x] Sequential/Batch Processing
	- [x] Perceptron Convergence
	- [x] Principal Component Analysis
	- [x] Dimensionality Reduction
	- [x] Radial Basis Functions
	- [x] Statistical tools for dimensionality reduction
	- [x] P-value test
	- [x] t-test
	- [x] Soft Max
	- [x] Cross Entropy
- [x] **Regularization**:
	- [x] Overview
	- [x] Parameter Penalties
	- [x] Norm Penalties as Constrained Optimization
	- [x] Regularization and Under-constrained Problems
	- [x] Data Augmentation
	- [x] Noise Robustness
	- [x] Batch Normalization
	- [x] Semi-Supervised Learning
	- [x] Multi-Task Learning
	- [x] Early Stopping
	- [x] Parameter Tying and Parameter Sharing
	- [x] Sparse Representations
	- [x] Bagging
	- [x] Dropout
	- [x] Tuning Neural Networks
	- [x] Hyperparameters
- [ ] **Convolutional Neural Networks**:
	- [x] Introduction
	- [x] Mechanics of Convolution Operation
	- [x] Pseudo Code of Convolution operation
	- [x] Convolution equation
	- [x] Zero Padding
	- [x] Vectorization of Convolution operation
	- [x] Convolution as Matrix multiplication
	- [x] Weight Sharing
	- [x] Translational Invariance
	- [x] Convolution on color images
	- [x] Convolution layer in CNN
	- [x] ReLU Layer
	- [x] Pooling
	- [x] Flattening
	- [x] Full Connection
	- [ ] Convolution and Back-propagation